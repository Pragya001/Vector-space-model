{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector_space_model:\n",
    "    \n",
    "    def start(self, cells):\n",
    "        hindi_document = []\n",
    "        for row in cells.iter_rows():\n",
    "            index = 0;\n",
    "            for k in row:\n",
    "                if index == 1:\n",
    "                    hindi_document.append(k.internal_value.lower())\n",
    "                    break\n",
    "                index = index + 1\n",
    "        \n",
    "        return hindi_document\n",
    "\n",
    "    def generate_tokens(self, hindi_document):\n",
    "        final_tokens = []\n",
    "        for i in range(len(hindi_document)):\n",
    "\n",
    "            nltk_tokens = nltk.word_tokenize(hindi_document[i])\n",
    "            #print(nltk_tokens)\n",
    "            final_tokens.append(nltk_tokens)\n",
    "        #rint(final_tokens)\n",
    "        return final_tokens\n",
    "\n",
    "    def remove_special_words(self, tokens):\n",
    "        final_tokens = []\n",
    "        special_characters = [',', ':', '-', '.', '(', ')', '[', ']', '{', '}', '|', '!', '@', '#', '$', '``','%', '^', '&', '*', '`', '~', ';', '<', '>', '|', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ii', 'iii']\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            temp = []\n",
    "            for j in range(len(tokens[i])):\n",
    "                word = tokens[i][j].strip(' ')\n",
    "\n",
    "                if word not in special_characters:\n",
    "                    if word.isdigit():\n",
    "                        t = word\n",
    "                    elif word != \"\": \n",
    "                        temp.append(word)\n",
    "\n",
    "            final_tokens.append(temp)\n",
    "\n",
    "        return final_tokens    \n",
    "    \n",
    "    def remove_stop_words(self, pruned_token):\n",
    "        f=codecs.open(\"stopwords.txt\",encoding='utf-8')\n",
    "        little_pruned_tokens = []\n",
    "\n",
    "        stopwords = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        for i in range(len(pruned_token)):\n",
    "            temp = []\n",
    "            for j in range(len(pruned_token[i])):\n",
    "                if pruned_token[i][j] not in stopwords:\n",
    "                    temp.append(pruned_token[i][j])    \n",
    "\n",
    "            little_pruned_tokens.append(temp)\n",
    "\n",
    "\n",
    "        return little_pruned_tokens    \n",
    "\n",
    "    def generate_stem_words(self, word):\n",
    "        suffixes = {\n",
    "            1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
    "            2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
    "            3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
    "            4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
    "            5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
    "        }\n",
    "\n",
    "        for L in 5, 4, 3, 2, 1:\n",
    "            if len(word) > L + 1:\n",
    "                 for suf in suffixes[L]:\n",
    "                    if word.endswith(suf):\n",
    "                        return word[:-L]\n",
    "        return word\n",
    "\n",
    "    def stemming(self, little_tokens):\n",
    "\n",
    "        stemmed_words = []\n",
    "\n",
    "        for i in range(len(little_tokens)):\n",
    "            temp = []\n",
    "            for j in range(len(little_tokens[i])):        \n",
    "                temp.append(self.generate_stem_words(little_tokens[i][j]))\n",
    "\n",
    "            stemmed_words.append(temp)\n",
    "\n",
    "        return stemmed_words\n",
    "\n",
    "    def frequency_count(self, stemmed_words):\n",
    "    \n",
    "        dictionary = {}\n",
    "\n",
    "        for i in range(len(stemmed_words)):\n",
    "            for j in range(len(stemmed_words[i])):        \n",
    "                count = dictionary.get(stemmed_words[i][j],0)\n",
    "                dictionary[stemmed_words[i][j]] = count + 1\n",
    "        \n",
    "        sorted_dict = self.sortedDictkeys(dictionary)\n",
    "        \n",
    "        return sorted_dict\n",
    "    \n",
    "    def vector_of_words(self, sorted_dict):\n",
    "    \n",
    "        word_vector = {}\n",
    "        k = 0\n",
    "        \n",
    "        for (key, value) in sorted_dict.items():\n",
    "            word_vector[key] = k\n",
    "            k = k+1\n",
    "            \n",
    "        return word_vector\n",
    "    \n",
    "    def sortedDictkeys(self, adict):\n",
    "        sort_dict = {}\n",
    "\n",
    "        for (key, value) in sorted(adict.items()):\n",
    "            sort_dict[key] = value\n",
    "\n",
    "        return sort_dict\n",
    "    \n",
    "    \n",
    "    def inverted_index_fun(self, documents):\n",
    "\n",
    "        inverted_index = defaultdict(set)\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            for j in range(len(documents[i])):\n",
    "\n",
    "                inverted_index[documents[i][j]].add(i)\n",
    "\n",
    "        #sorting the dictionary by values:\n",
    "        for keys, value in inverted_index.items():\n",
    "            tmp = sorted(value)\n",
    "            inverted_index[keys] = tmp\n",
    "\n",
    "        return inverted_index\n",
    "    \n",
    "    def weighted_term_doc_matrix(self, stemmed_docs, word_vector):\n",
    "        \n",
    "        #-------- creating term matrix of all documents with dim (no of word * no of docs) ---------------#\n",
    "        \n",
    "        tf_matrix = np.zeros((len(word_vector), len(stemmed_docs)), dtype=int)\n",
    "        \n",
    "        #-------- updating term matrix ------------------------------------------------------------------#\n",
    "        \n",
    "        for i in range(len(stemmed_docs)):\n",
    "            for j in range(len(stemmed_docs[i])):\n",
    "                word = stemmed_docs[i][j]\n",
    "                index = word_vector[word]\n",
    "                tf_matrix[index][i] = tf_matrix[index][i] + 1\n",
    "        \n",
    "        #-------- calculating idf of terms -------------------------------------------------------------#\n",
    "        nonzero_count = np.count_nonzero(tf_matrix, axis=1)\n",
    "        N = len(stemmed_docs)\n",
    "        idf = np.log(N/nonzero_count)\n",
    "        tf_matrix = (tf_matrix.T * idf).T\n",
    "\n",
    "        #-------- normalizing all the document vectors -------------------------------------------------#\n",
    "        normalized_vec = np.sqrt(np.sum(np.square(tf_matrix), axis=0))\n",
    "        tf_matrix = tf_matrix/normalized_vec\n",
    "        \n",
    "        \n",
    "        return tf_matrix, idf\n",
    "    \n",
    "    def query_vector(self, stemmed_query, word_vector, idf):\n",
    "        \n",
    "        #-------- creating query matrix of all queris with dim (no of word * no of queries) ---------------#\n",
    "        \n",
    "        query_matrix = np.zeros((len(word_vector), len(stemmed_query)), dtype=int)\n",
    "        \n",
    "        #-------- updating query matrix ------------------------------------------------------------------#\n",
    "        \n",
    "        for i in range(len(stemmed_query)):\n",
    "            for j in range(len(stemmed_query[i])):\n",
    "                word = stemmed_query[i][j]\n",
    "                if word in word_vector.keys(): \n",
    "                    index = word_vector[word]\n",
    "                    query_matrix[index][i] = query_matrix[index][i] + 1\n",
    "    \n",
    "        #------- Updating query matrix by multiplying idf to it -----------------------------------------#\n",
    "        \n",
    "        query_matrix = (query_matrix.T * idf).T\n",
    "        \n",
    "        #------- Normalizing all the query vectors ------------------------------------------------------#\n",
    "        normalized_query = query_matrix/(np.sqrt(np.sum(np.square(query_matrix), axis=0)))\n",
    "        \n",
    "        \n",
    "        return normalized_query\n",
    "    \n",
    "    def query_doc_similarity(self, weighted_matrix, query_matrix):\n",
    "        \n",
    "        cosine_val = np.dot(weighted_matrix.T, query_matrix)\n",
    "        \n",
    "        return cosine_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl as px\n",
    "import codecs\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    W = px.load_workbook('Dand_Prakriya.xlsx')\n",
    "    p = W.get_sheet_by_name(name = 'Sheet')\n",
    "    hindi_document = []\n",
    "    \n",
    "    ret = Vector_space_model()\n",
    "    \n",
    "    #---------------- Pre-processing the documents by removing stopwords and stemming --------------------------------#\n",
    "    \n",
    "    hindi_document = ret.start(p)\n",
    "    tokens = ret.generate_tokens(hindi_document)\n",
    "    pruned_tokens = ret.remove_special_words(tokens)\n",
    "    little_tokens = ret.remove_stop_words(pruned_tokens)\n",
    "    stemmed_words = ret.stemming(little_tokens)\n",
    "    \n",
    "    \n",
    "    #--------------- Creating Inverted index file --------------------------------------------------------------------#\n",
    "    \n",
    "    inverted_index = ret.inverted_index_fun(stemmed_words)\n",
    "    inv_file = open(\"inverted_index_hin.txt\",\"w\")\n",
    "    inv_file.write(str(inverted_index))\n",
    "    inv_file.close()\n",
    "    \n",
    "    \n",
    "    #--------------- Creating Bag of words and their frequency file --------------------------------------------------#\n",
    "    \n",
    "    frequency = ret.frequency_count(stemmed_words)\n",
    "    freq_file = open(\"OUT_hin_freq.txt\",\"w\")\n",
    "    freq_file.write(str(frequency))\n",
    "    freq_file.close()\n",
    "    word_vector = ret.vector_of_words(frequency)\n",
    "    \n",
    "    #--------------- Creating the Weighted term-idf matrix and normalizing all the document vectors ------------------#\n",
    "    \n",
    "    normalized_matrix, idf = ret.weighted_term_doc_matrix(stemmed_words, word_vector)\n",
    "    \n",
    "    #--------------- storing weighted normalized term-idf matrix in excel sheet --------------------------------------#\n",
    "    \n",
    "    df = pd.DataFrame(normalized_matrix)\n",
    "    df = df.iloc[76:]\n",
    "    df.insert(0, \"words\", word_vector, True)\n",
    "    doc = ['words']\n",
    "    s = \"\"\n",
    "    for i in range(len(tokens)):\n",
    "        s = 'doc ' + str(i+1)\n",
    "        doc.append(s)\n",
    "    df.columns = doc\n",
    "    \n",
    "    filepath = 'Excel_hin.xlsx'\n",
    "    df.to_excel(filepath, index=False)\n",
    "    \n",
    "    \n",
    "    #--------------- Processing the Queries and calculating the cosine similarity between queries and documents --------#\n",
    "    \n",
    "    f=codecs.open(\"query_hin.txt\",encoding='utf-8')\n",
    "    queries = [x.strip() for x in f.readlines()]    \n",
    "    query_tokens = ret.generate_tokens(queries)\n",
    "    query_prun_tokens = ret.remove_special_words(query_tokens)\n",
    "    query_pruned_tokens = ret.remove_stop_words(query_prun_tokens)\n",
    "    query_stemmed_words = ret.stemming(query_pruned_tokens)\n",
    "    normalized_query = ret.query_vector(query_stemmed_words, word_vector, idf)\n",
    "    \n",
    "    cosine_val = ret.query_doc_similarity(normalized_matrix, normalized_query)\n",
    "    \n",
    "    std_out = sys.stdout\n",
    "    file = open('OUT_HINDI.txt', 'w')\n",
    "    sys.stdout = file\n",
    "\n",
    "    for i in range(len(queries)):\n",
    "        doc = []\n",
    "        print(queries[i], ':')\n",
    "        for j in range(len(cosine_val)):\n",
    "            if cosine_val[j][i] >= 0.65:\n",
    "                doc.append(j+1)\n",
    "        print(doc)\n",
    "                \n",
    "    sys.stdout = std_out\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dell/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:160: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/dell/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:160: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
